import pickle
import numpy as np
import torch
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors

# ========= åŠ è½½ vocab å’Œ merges =========
with open('save/tokenizer_vocab_train.pkl', 'rb') as f:
    vocab = pickle.load(f)

with open('save/tokenizer_merges_train.pkl', 'rb') as f:
    merges = pickle.load(f)

special_tokens = ["<|endoftext|>"]

print(f"åŠ è½½ vocab({len(vocab)}) å’Œ merges({len(merges)}) å®Œæˆ")

# ========= æ„å»º HuggingFace BPE Tokenizer =========
# vocab: {id: bytes}ï¼Œè€Œ HF éœ€è¦ {token_str: id}
# å› æ­¤éœ€è¦è¿›è¡Œä¸€æ¬¡åè½¬å’Œè§£ç 
vocab_str = {v.decode("latin-1"): k for k, v in vocab.items()}
"""
mergesé‡Œé¢æœ‰è¿™ç§æ²¡æ³•decodeçš„å­—ç¬¦
(b'\xe2', b'\x80')
"""
merges_str = []
print("å‰å‡ ä¸ª merges ç¤ºä¾‹:", merges[440:442])
step = 0
for merge in merges:
    if isinstance(merge, tuple) and len(merge) == 2:
        step = step + 1
        # å°† bytes è§£ç ä¸ºå­—ç¬¦ä¸²
        try:
            token1 = (merge[0].decode("latin-1"))
            token2 = (merge[1].decode("latin-1"))
        except Exception as e:
            print(f"å‘ç”Ÿäº†æœªçŸ¥é”™è¯¯ï¼š{e}")
            print("step: ")
            print(step)
        merges_str.append((token1, token2))
    else:
        print(f"è·³è¿‡ä¸æ”¯æŒçš„ merge æ ¼å¼: {merge}")

print(f"è½¬æ¢åçš„ merges æ•°é‡: {len(merges_str)}")
print("å‰å‡ ä¸ª merges ç¤ºä¾‹:", merges_str[440:442])



tokenizer = Tokenizer(models.BPE(vocab=vocab_str, merges=merges_str))
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
tokenizer.decoder = decoders.ByteLevel()
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
# tokenizer.enable_truncation(None)

# ä¿å­˜ä¸º HuggingFace æ ‡å‡†æ ¼å¼ï¼ˆå¯é€‰ï¼‰
tokenizer.save("save/my_tokenizer.json")
print("åˆå§‹åŒ– HuggingFace tokenizer å®Œæˆ âœ…")

# ========= ç¼–ç å‡½æ•° =========
def encode_large_text_file(file_path, output_path, chunk_size=10000):
    """
    æµå¼è¯»å–å¤§æ–‡ä»¶å¹¶ç¼–ç ä¿å­˜ä¸º npy
    """
    all_ids = []
    with open(file_path, "r", encoding="utf-8") as f:
        chunk = []
        for i, line in enumerate(f):
            chunk.append(line.strip())
            if len(chunk) >= chunk_size:
                text_chunk = " ".join(chunk)
                encodings = tokenizer.encode(text_chunk)
                all_ids.extend(encodings.ids)
                chunk = []
                print(f"å·²å¤„ç† {i+1} è¡Œ")

        if chunk:
            text_chunk = " ".join(chunk)
            encodings = tokenizer.encode(text_chunk)
            all_ids.extend(encodings.ids)

    arr = np.array(all_ids, dtype=np.int64)
    np.save(output_path, arr)
    print(f"âœ… å·²ä¿å­˜ {output_path} ï¼ˆ{len(all_ids)} tokensï¼‰")

# ========= å¤„ç† train æ•°æ® =========
train_path = "data/TinyStoriesV2-GPT4-train.txt"
encode_large_text_file(train_path, "save/encode_ids_train.npy")

# ========= å¤„ç† valid æ•°æ® =========
# valid_path = "data/TinyStoriesV2-GPT4-valid.txt"
# with open(valid_path, "r", encoding="utf-8") as f:
#     text = f.read()
#     encodings = tokenizer.encode(text)
#     ids = np.array(encodings.ids, dtype=np.int64)
#     np.save("save/encode_ids_valid.npy", ids)
#     print(f"âœ… valid æ•°æ®å·²ä¿å­˜ï¼Œå…± {len(ids)} tokens")

print("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")
