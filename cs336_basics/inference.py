import torch

import torch
import torch.nn.functional as F
import numpy as np

from cs336_basics.tokenizer_two import Tokenizer

def top_p_sampling(logits, p=0.9, temperature=1.0):
    """
    Top-p (nucleus) é‡‡æ ·
    
    Args:
        logits: æ¨¡å‹è¾“å‡ºçš„logitså¼ é‡ [vocab_size]
        p: top-pé˜ˆå€¼ (0-1ä¹‹é—´)
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
    Returns:
        selected_token: é‡‡æ ·å¾—åˆ°çš„tokenç´¢å¼•
    """
    # åº”ç”¨æ¸©åº¦å‚æ•°
    logits = logits / temperature
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # å¯¹æ¦‚ç‡æ’åºï¼ˆé™åºï¼‰
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    
    # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„ç¬¬ä¸€ä¸ªä½ç½®
    # æˆ‘ä»¬è¦ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„tokenï¼Œæ‰€ä»¥æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¶…è¿‡pçš„ä½ç½®
    sorted_indices_to_remove = cumulative_probs > p
    
    # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªtokenï¼ˆå¦‚æœæ‰€æœ‰tokenæ¦‚ç‡éƒ½å¾ˆå°ï¼‰
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    # è·å–è¦ç§»é™¤çš„tokenç´¢å¼•
    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    
    # å°†è¿™äº›tokençš„æ¦‚ç‡è®¾ä¸ºå¾ˆå°çš„å€¼
    filtered_logits = logits.clone()
    filtered_logits[indices_to_remove] = -float('inf')
    
    # é‡æ–°è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
    filtered_probs = F.softmax(filtered_logits, dim=-1)
    
    # ä»è¿‡æ»¤åçš„åˆ†å¸ƒä¸­é‡‡æ ·
    selected_token = torch.multinomial(filtered_probs, num_samples=1)
    
    return selected_token.item()

def top_p_sampling_batch(logits, p=0.9, temperature=1.0):
    """
    æ‰¹é‡å¤„ç†çš„Top-pé‡‡æ ·
    
    Args:
        logits: [batch_size, vocab_size]
        p: top-pé˜ˆå€¼
        temperature: æ¸©åº¦å‚æ•°
    Returns:
        selected_tokens: [batch_size] é‡‡æ ·å¾—åˆ°çš„token
    """
    batch_size, vocab_size = logits.shape
    
    # åº”ç”¨æ¸©åº¦å‚æ•°
    logits = logits / temperature
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å¤„ç†
    selected_tokens = []
    for i in range(batch_size):
        # å¯¹å•ä¸ªæ ·æœ¬çš„æ¦‚ç‡æ’åº
        sorted_probs, sorted_indices = torch.sort(probs[i], descending=True)
        
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # æ‰¾åˆ°è¦ç§»é™¤çš„token
        sorted_indices_to_remove = cumulative_probs > p
        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
        sorted_indices_to_remove[0] = False
        
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        
        # è¿‡æ»¤æ¦‚ç‡
        filtered_probs = probs[i].clone()
        filtered_probs[indices_to_remove] = 0
        
        # é‡æ–°å½’ä¸€åŒ–
        if filtered_probs.sum() > 0:
            filtered_probs = filtered_probs / filtered_probs.sum()
        else:
            # å¦‚æœæ‰€æœ‰æ¦‚ç‡éƒ½è¢«è¿‡æ»¤ï¼Œå›é€€åˆ°åŸå§‹åˆ†å¸ƒ
            filtered_probs = probs[i]
        
        # é‡‡æ ·
        selected_token = torch.multinomial(filtered_probs.unsqueeze(0), num_samples=1)
        selected_tokens.append(selected_token.item())
    
    return torch.tensor(selected_tokens)

# # æµ‹è¯•æ•°æ® - åˆ›å»ºä¸€ä¸ªç®€å•çš„logitså¼ é‡
# torch.manual_seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°ç»“æœ
# vocab_size = 10
# test_logits = torch.randn(vocab_size) * 2  # éšæœºç”Ÿæˆlogits

# print("æµ‹è¯•logits:", test_logits)
# print("åŸå§‹æ¦‚ç‡åˆ†å¸ƒ:", F.softmax(test_logits, dim=-1))

# # æµ‹è¯•top-pé‡‡æ ·
# selected_token = top_p_sampling(test_logits, p=0.9, temperature=1.0)
# print(f"\né‡‡æ ·ç»“æœ: token_id = {selected_token}")

# # å¤šæ¬¡é‡‡æ ·è§‚å¯Ÿåˆ†å¸ƒ
# print("\nå¤šæ¬¡é‡‡æ ·ç»“æœ:")
# for i in range(10):
#     token_id = top_p_sampling(test_logits, p=0.9, temperature=1.0)
#     print(f"ç¬¬{i+1}æ¬¡é‡‡æ ·: {token_id}")

# # å¯è§†åŒ–top-pè¿‡æ»¤æ•ˆæœ
# print("\n=== Top-pè¿‡æ»¤æ•ˆæœåˆ†æ ===")
# probs = F.softmax(test_logits, dim=-1)
# sorted_probs, sorted_indices = torch.sort(probs, descending=True)
# cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

# print("æ’åºåçš„æ¦‚ç‡:", sorted_probs)
# print("ç´¯ç§¯æ¦‚ç‡:", cumulative_probs)
# print("ä¿ç•™çš„tokenç´¢å¼•:", sorted_indices[cumulative_probs <= 0.9])

def generate_text(model, tokenizer: Tokenizer, prompt: str, max_length: int=256, p: float=0.9, 
                  temperature: float=1.0, device="cuda" if torch.cuda.is_available() else "cpu", special_tokens = "<|endoftext|>"):
    """
    æ–‡æœ¬ç”Ÿæˆå‡½æ•°
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        prompt: è¾“å…¥æ–‡æœ¬
        max_length: ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        p: top-pé˜ˆå€¼
        temperature: æ¸©åº¦å‚æ•°
        device: è®¾å¤‡
    Returns:
        generated_text: ç”Ÿæˆçš„æ–‡æœ¬
    """
    model.eval()
    
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    input_ids = torch.tensor(tokenizer.encode(prompt)).to(device)
    print("max input_id: ")
    print(max(input_ids))
    prompt_length = len(input_ids)
    if input_ids.dim() == 1:  # å¦‚æœæ˜¯ä¸€ç»´çš„ [seq_len]
        input_ids = input_ids.unsqueeze(0)  # å˜æˆ [1, seq_len]
    generated = input_ids.clone()
    
    print(f"å¼€å§‹ç”Ÿæˆï¼Œåˆå§‹è¾“å…¥: '{prompt}'")
    print(f"è¾“å…¥tokenæ•°: {len(input_ids[0])}")
    
    with torch.no_grad():
        for step in range(max_length):
            # è·å–æ¨¡å‹è¾“å‡º
            outputs = model(input_ids)
            logits = outputs[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„logits
            
            # ä½¿ç”¨top-pé‡‡æ ·é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
            next_token_id = top_p_sampling(logits[0], p=p, temperature=temperature)
            
            # å°†æ–°tokenæ·»åŠ åˆ°åºåˆ—ä¸­
            next_token = torch.tensor([[next_token_id]]).to(device)
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            generated = torch.cat([generated, next_token], dim=-1)
            
            # æ‰“å°å½“å‰ç”Ÿæˆçš„tokenï¼ˆå¯é€‰ï¼‰
            # print(f"æ­¥éª¤ {step+1}: token_id={next_token_id}, text='{tokenizer.decode(next_token_id)}'")
            
            # å¦‚æœé‡åˆ°ç»“æŸç¬¦ï¼Œæå‰åœæ­¢
            special_token_ids = [tokenizer.encode(special_token) for special_token in special_tokens]
            if next_token_id in special_token_ids:
                break
                
            # é™åˆ¶è¾“å…¥é•¿åº¦ï¼ˆæœ€å¤šå’Œcontext_lengthä¸€æ ·ï¼ å› ä¸ºropeçš„çŸ©é˜µå¤§å°ä¸ºcontext_length * context_length,
            # å‡å¦‚ä¸é™åˆ¶çš„è¯å°±ä¼šè®¿é—®ropeçŸ©é˜µæ•°ç»„è¶Šç•Œï¼‰
            if input_ids.shape[-1] > 256:
                input_ids = input_ids[:, -256:]
    
    # è§£ç ç”Ÿæˆç»“æœ
    new_tokens = generated[0]
    # new_tokens = generated[0][prompt_length:]  # å»æ‰promptå¯¹åº”çš„token
    # å°†CUDA tensorè½¬æ¢ä¸ºCPUä¸Šçš„æ™®é€šPythonåˆ—è¡¨
    new_tokens_list = new_tokens.cpu().tolist()
    generated_text = tokenizer.decode(new_tokens_list)
    return generated_text

# æ›´é€šç”¨çš„ç‰ˆæœ¬ï¼Œæ”¯æŒä¸åŒçš„åœæ­¢æ¡ä»¶
def generate_autoregressive(model, tokenizer, prompt, max_length=50, p=0.9, temperature=1.0, 
                          stop_tokens=None, device='cpu'):
    """
    è‡ªå›å½’ç”Ÿæˆå‡½æ•°
    
    Args:
        model: è¯­è¨€æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        prompt: è¾“å…¥æ–‡æœ¬
        max_length: ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        p: top-pé˜ˆå€¼
        temperature: æ¸©åº¦å‚æ•°
        stop_tokens: åœæ­¢tokenåˆ—è¡¨
        device: è®¾å¤‡
    Returns:
        generated_text: ç”Ÿæˆçš„æ–‡æœ¬
        generated_ids: ç”Ÿæˆçš„token IDs
    """
    model.eval()
    
    if stop_tokens is None:
        stop_tokens = [tokenizer.eos_token_id]
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    generated_ids = input_ids[0].tolist()
    
    print(f"ç”Ÿæˆå¼€å§‹: '{prompt}'")
    
    with torch.no_grad():
        for step in range(max_length):
            # è·å–å½“å‰è¾“å…¥ï¼ˆåªä¿ç•™æœ€è¿‘çš„éƒ¨åˆ†ä»¥é¿å…è¿‡é•¿ï¼‰
            current_input = input_ids[:, -min(512, input_ids.shape[1]):]
            
            # æ¨¡å‹å‰å‘ä¼ æ’­
            outputs = model(current_input)
            logits = outputs.logits[:, -1, :]
            
            # top-pé‡‡æ ·
            next_token_id = top_p_sampling(logits[0], p=p, temperature=temperature)
            
            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated_ids.append(next_token_id)
            
            # æ›´æ–°è¾“å…¥ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
            next_token = torch.tensor([[next_token_id]]).to(device)
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if next_token_id in stop_tokens:
                break
    
    # è§£ç 
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return generated_text, generated_ids

# æµå¼ç”Ÿæˆç‰ˆæœ¬ï¼ˆå®æ—¶è¾“å‡ºï¼‰
def generate_streaming(model, tokenizer, prompt, max_length=50, p=0.9, temperature=1.0, device='cpu'):
    """
    æµå¼ç”Ÿæˆï¼Œå®æ—¶è¾“å‡ºç»“æœ
    """
    model.eval()
    
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    generated_text = prompt
    
    print(f"è¾“å…¥: {prompt}")
    print("ç”Ÿæˆ: ", end="", flush=True)
    
    with torch.no_grad():
        for step in range(max_length):
            current_input = input_ids[:, -512:]  # æ»‘åŠ¨çª—å£
            
            outputs = model(current_input)
            logits = outputs.logits[:, -1, :]
            
            next_token_id = top_p_sampling(logits[0], p=p, temperature=temperature)
            
            # è§£ç å¹¶æ‰“å°å½“å‰token
            next_token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)
            print(next_token_text, end="", flush=True)
            
            generated_text += next_token_text
            
            # æ›´æ–°è¾“å…¥
            next_token = torch.tensor([[next_token_id]]).to(device)
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            
            if next_token_id == tokenizer.eos_token_id:
                break
    
    print()  # æ¢è¡Œ
    return generated_text

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆä¼ªä»£ç ï¼‰
def example_usage():
    """
    ä½¿ç”¨ç¤ºä¾‹ - éœ€è¦å®é™…çš„modelå’Œtokenizer
    """
    # å‡è®¾æˆ‘ä»¬æœ‰modelå’Œtokenizer
    # from transformers import AutoModel, AutoTokenizer
    # model = AutoModel.from_pretrained("your-model")
    # tokenizer = AutoTokenizer.from_pretrained("your-model")
    
    prompt = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ"
    
    # åŸºæœ¬ç”Ÿæˆ
    # result = generate_text(model, tokenizer, prompt, max_length=50, p=0.9, temperature=0.8)
    
    # æµå¼ç”Ÿæˆ
    # result = generate_streaming(model, tokenizer, prompt, max_length=50, p=0.9, temperature=0.8)
    
    # å¸¦åœæ­¢è¯çš„ç”Ÿæˆ
    # stop_tokens = [tokenizer.eos_token_id, tokenizer.encode("ã€‚")[0]]
    # result, ids = generate_autoregressive(model, tokenizer, prompt, stop_tokens=stop_tokens)
    
    print("ç”Ÿæˆå®Œæˆ")

# æµ‹è¯•ç”¨çš„ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸éœ€è¦çœŸå®æ¨¡å‹ï¼‰
def test_generation():
    """
    æµ‹è¯•ç”Ÿæˆå‡½æ•° - ä½¿ç”¨æ¨¡æ‹Ÿçš„logits
    """
    class MockModel:
        def __init__(self, vocab_size=1000):
            self.vocab_size = vocab_size
            
        def __call__(self, input_ids):
            # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º - éšæœºlogits
            batch_size, seq_len = input_ids.shape
            logits = torch.randn(batch_size, seq_len, self.vocab_size)
            return type('Output', (), {'logits': logits})()
    
    class MockTokenizer:
        def __init__(self):
            self.vocab_size = 1000
            self.eos_token_id = 999
            
        def encode(self, text, return_tensors=None):
            # æ¨¡æ‹Ÿç¼–ç  - è¿”å›éšæœºtoken IDs
            ids = [1, 2, 3, 4, 5]  # æ¨¡æ‹Ÿè¾“å…¥åºåˆ—
            if return_tensors == 'pt':
                return torch.tensor([ids])
            return ids
            
        def decode(self, ids, skip_special_tokens=True):
            # æ¨¡æ‹Ÿè§£ç 
            return f"[æ¨¡æ‹Ÿæ–‡æœ¬: {len(ids)}ä¸ªtoken]"
    
    # æµ‹è¯•
    model = MockModel()
    tokenizer = MockTokenizer()
    
    prompt = "æµ‹è¯•è¾“å…¥"
    result = generate_text(model, tokenizer, prompt, max_length=10, p=0.9, temperature=1.0)
    print(f"æµ‹è¯•ç”Ÿæˆç»“æœ: {result}")

def generate_text_with_debug(model, tokenizer: Tokenizer, prompt: str, max_length: int=256, p: float=0.9, 
                  temperature: float=1.0, device="cuda" if torch.cuda.is_available() else "cpu", 
                  special_tokens = "<|endoftext|>"):
    """
    æ–‡æœ¬ç”Ÿæˆå‡½æ•° - è°ƒè¯•ç‰ˆæœ¬ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªè¯å°±è¾“å‡º
    """
    model.eval()
    
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    input_ids = torch.tensor(tokenizer.encode(prompt)).to(device)
    print(f"è¾“å…¥tokenæ•°: {len(input_ids)}")
    print(f"è¾“å…¥token IDs: {input_ids.tolist()}")
    
    # æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦è¶…è¿‡æ¨¡å‹é™åˆ¶
    if len(input_ids) > 256:
        print(f"è­¦å‘Š: è¾“å…¥åºåˆ—é•¿åº¦ {len(input_ids)} è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ {256}")
        input_ids = input_ids[:256]
        print(f"æˆªæ–­åtoken IDs: {input_ids.tolist()}")
    
    if input_ids.dim() == 1:
        input_ids = input_ids.unsqueeze(0)
    
    generated = input_ids.clone()
    prompt_length = len(input_ids[0])
    
    print(f"å¼€å§‹ç”Ÿæˆï¼Œåˆå§‹è¾“å…¥: '{prompt}'")
    print("=" * 50)
    
    with torch.no_grad():
        for step in range(max_length):
            # æ£€æŸ¥å½“å‰åºåˆ—é•¿åº¦
            current_length = input_ids.shape[1]
            if current_length > 256:
                # æ»‘åŠ¨çª—å£ï¼šä¿ç•™æœ€è¿‘çš„context_lengthä¸ªtoken
                input_ids = input_ids[:, -256:]
                print(f"æ­¥éª¤ {step}: åºåˆ—æˆªæ–­åˆ° {256}")
            
            print(f"\n--- æ­¥éª¤ {step+1} ---")
            print(f"å½“å‰è¾“å…¥åºåˆ—é•¿åº¦: {input_ids.shape[1]}")
            
            try:
                # è·å–æ¨¡å‹è¾“å‡º
                print("æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­...")
                outputs = model(input_ids)
                logits = outputs[:, -1, :]
                print(f"Logitså½¢çŠ¶: {logits.shape}")
                print(f"LogitsèŒƒå›´: {logits.min():.3f} ~ {logits.max():.3f}")
                
                # ä½¿ç”¨top-pé‡‡æ ·é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
                print("æ‰§è¡Œtop-pé‡‡æ ·...")
                next_token_id = top_p_sampling(logits[0], p=p, temperature=temperature)
                print(f"é‡‡æ ·å¾—åˆ°çš„token ID: {next_token_id}")
                
                # è§£ç å½“å‰token
                current_token_text = tokenizer.decode([next_token_id])
                print(f"ç”Ÿæˆçš„tokenæ–‡æœ¬: '{current_token_text}'")
                
                # å°†æ–°tokenæ·»åŠ åˆ°åºåˆ—ä¸­
                next_token = torch.tensor([[next_token_id]]).to(device)
                input_ids = torch.cat([input_ids, next_token], dim=-1)
                generated = torch.cat([generated, next_token], dim=-1)
                
                # æ˜¾ç¤ºå½“å‰å®Œæ•´ç”Ÿæˆç»“æœ
                current_full_text = tokenizer.decode(generated[0].cpu().tolist())
                print(f"å½“å‰å®Œæ•´æ–‡æœ¬: '{current_full_text}'")
                
                # å¦‚æœé‡åˆ°ç»“æŸç¬¦ï¼Œæå‰åœæ­¢
                if special_tokens:
                    special_token_id = tokenizer.encode(special_tokens)[0]
                    if next_token_id == special_token_id:
                        print("ğŸ¯ é‡åˆ°ç»“æŸç¬¦ï¼Œåœæ­¢ç”Ÿæˆ")
                        break
                        
            except Exception as e:
                print(f"âŒ æ­¥éª¤ {step+1} å‡ºé”™: {e}")
                print(f"å‡ºé”™æ—¶çš„è¾“å…¥IDs: {input_ids.cpu().tolist()}")
                print(f"å‡ºé”™æ—¶çš„ç”ŸæˆIDs: {generated.cpu().tolist()}")
                import traceback
                traceback.print_exc()
                break
    
    # è§£ç æœ€ç»ˆç”Ÿæˆç»“æœ
    final_tokens = generated[0].cpu().tolist()
    generated_text = tokenizer.decode(final_tokens)
    
    print("=" * 50)
    print("ğŸ‰ ç”Ÿæˆå®Œæˆ!")
    print(f"æœ€ç»ˆç»“æœ: '{generated_text}'")
    
    return generated_text

if __name__ == "__main__":
    test_generation()