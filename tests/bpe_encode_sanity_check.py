# é€‰æ‹©ä¸€äº›æ ·æœ¬æ–‡æœ¬è¿›è¡Œç¼–ç -è§£ç æµ‹è¯•
from cs336_basics.tokenizer_two import Tokenizer
import pickle

# Round-trip Test
supports_chinese = False
sample_texts = [
       # åŸºç¡€è‹±æ–‡
    "Hello world!",
    "The quick brown fox jumps over the lazy dog.",
    
    # ä¸­æ–‡æµ‹è¯•ï¼ˆå¦‚æœæ”¯æŒï¼‰
    "æµ‹è¯•ä¸­æ–‡" if supports_chinese else "",
    "ä½ å¥½ä¸–ç•Œï¼è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬ã€‚" if supports_chinese else "",
    
    # ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹
    "Hello, world! How are you?",
    "I'm feeling great!",
    "This costs $100.99.",
    "Email: test@example.com",
    
    # æ•°å­—å’Œç¬¦å·
    "1234567890",
    "1 + 2 = 3",
    "Special chars: !@#$%^&*()",
    
    # ç©ºæ ¼å’Œæ ¼å¼
    "Text with     multiple   spaces",
    "Line1\nLine2\nLine3",
    "Tab\tseparated",
    
    # è¾¹ç•Œæƒ…å†µ
    "",  # ç©ºå­—ç¬¦ä¸²
    " ",  # åªæœ‰ç©ºæ ¼
    "a",  # å•ä¸ªå­—ç¬¦
    "A",  # å•ä¸ªå¤§å†™å­—ç¬¦
    
    # é‡å¤å­—ç¬¦
    "aaaaaaa",
    "!!!",
    
    # æ··åˆå†…å®¹
    "Hello 123 ä¸–ç•Œï¼",
    "Price: Â¥100.50" if supports_chinese else "Price: $100.50",
    
    # é•¿æ–‡æœ¬
    "This is a longer text that contains multiple sentences. "
    "It should test how the tokenizer handles extended input. "
    "Let's see if everything works correctly!",
    
    # URLå’Œè·¯å¾„
    "Visit https://example.com/path/to/page",
    "C:\\Users\\Documents\\file.txt",
    
    # è¡¨æƒ…ç¬¦å·ï¼ˆå¦‚æœtokenizeræ”¯æŒï¼‰
    "Hello ğŸ˜Š World ğŸŒ",
    
    # å¼•å·
    '"Quoted text" and \'single quotes\'',
    
    # æ‹¬å·
    "Text (with parentheses) [and brackets] {and braces}",
]

# === 1. åŠ è½½ tokenizer ===
with open('save/TinyStoriesV2-vocab.pkl', 'rb') as f:
    vocab = pickle.load(f)

with open('save/TinyStoriesV2-merges.pkl', 'rb') as f:
    merges = pickle.load(f)

special_tokens = "<|endoftext|>"
tokenizer = Tokenizer(vocab, merges, special_tokens)

for text in sample_texts:
    encoded = tokenizer.encode(text)
    decoded = tokenizer.decode(encoded)
    assert text == decoded, f"Round-trip failed: {text} != {decoded}"

print("Round-trip Test Success")

import numpy as np
from cs336_basics.tokenizer_two import Tokenizer
import pickle

# ä½¿ç”¨å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½ï¼Œä¸å®é™…è¯»å…¥å†…å­˜
encoded_data = np.load('save/TinyStoriesV2-GPT4-train.npy', mmap_mode='r')

# åŠ è½½tokenizer
with open('save/TinyStoriesV2-vocab.pkl', 'rb') as f:
    vocab = pickle.load(f)
with open('save/TinyStoriesV2-merges.pkl', 'rb') as f:
    merges = pickle.load(f)

special_tokens = "<|endoftext|>"
tokenizer = Tokenizer(vocab, merges, special_tokens)

# éšæœºæŠ½å–å¤šä¸ªæ ·æœ¬å¿«é€Ÿæ£€æŸ¥
import random

def quick_sample_check(num_samples=5, sample_length=50):
    print("Quick sampling check:")
    print("=" * 50)
    
    # è·å–æ–‡ä»¶å¤§å°ä»¥ç¡®å®šéšæœºèŒƒå›´
    file_size = len(encoded_data)
    print(f"Total tokens in file: {file_size:,}")
    
    for i in range(num_samples):
        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®ï¼Œé¿å…æ–‡ä»¶æœ«å°¾
        start_idx = random.randint(0, file_size - sample_length - 1)
        sample_tokens = encoded_data[start_idx:start_idx + sample_length]
        
        # è½¬æ¢ä¸ºPythonåˆ—è¡¨ï¼ˆå¦‚æœä½¿ç”¨å†…å­˜æ˜ å°„ï¼‰
        if hasattr(sample_tokens, 'tolist'):
            sample_tokens = sample_tokens.tolist()
        
        decoded_text = tokenizer.decode(sample_tokens)
        
        print(f"Sample {i+1} (position {start_idx:,}):")
        print(f"'{decoded_text}'")
        print("-" * 40)

# å¿«é€Ÿæ£€æŸ¥
quick_sample_check()